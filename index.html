<head>
<title> KIDD: Self-Supervised Decomposition, Disentanglement and Predictionof Video Sequences while Interpreting Dynamics:A Koopman Perspective </title>
<style> 
body{font-size:100%; /*32px verdana, arial, sans-serif;*/
    text-align:center;
    } 
#wrap{
    display: flex;
    justify-content: space-around;
}
.scrollmenu {
  background-color: #333;
  overflow: auto;
  white-space: nowrap;
}

.scrollmenu a {
  display: inline-block;
  color: white;
  text-align: center;
  padding: 20px;
  text-decoration: none;
}

.scrollmenu a:hover {
  background-color: #777;
}

.menu{
  	overflow: auto;
  	white-space: nowrap;
}
.menu a {
  color: blue;
  text-align: center;
  padding: 20px;
  text-decoration: none;
}
.menu a:hover {
  color:rgb(150,150,255);
}

.Row {
    display: table;
    width: 100%; /*Optional*/
    table-layout: fixed; /*Optional*/
    border-spacing: 10px; /*Optional*/
}
.Column {
    display: table-cell; /*Optional*/
}
.float-left {
    float:left;
    width:50px; /*// or 33% for equal width independent of parent width*/
    margin:5px
}


ul {
    font-size: 0px;
}

ul li {
    padding: 0px 20px;
    font-size: 12px;
    /*border: 1px solid #F00;*/
    display: inline-block;
}
</style> 
 
<!--<script defer src='./build/bundle.js'></script>-->    
    
</head>


<!-- define scrpit javascript -->
<script type="text/javascript">
function mousePosition(ev){
     if(ev.pageX || ev.pageY){
      return {x:ev.pageX, y:ev.pageY};
      }
      return {
       x:ev.clientX + document.body.scrollLeft - document.body.clientLeft,
       y:ev.clientY + document.body.scrollTop  - document.body.clientTop
       };
 }

function mouseMove(ev){
    ev = ev || window.event;
    var mousePos = mousePosition(ev);
    document.getElementById('xxx').value = mousePos.x;
    document.getElementById('yyy').value = mousePos.y;
}

document.onmousemove = mouseMove;
    
function mapOnClick(e){
        e = e || window.event;
        var imgId ='#'+ $(e.target).attr('id');
        var currentWidth = $(imgId).width();
        var currentHeight = $(imgId).height();
        var offsetX = e.pageX - $(imgId).offset().left;
        var offsetY = e.pageY - $(imgId).offset().top;
        var x = offsetX / currentWidth;
        var y = offsetY / currentHeight;
        alert(x + ':' + y);
    }
    
 function Show(el){
      var x = parseInt(document.getElementById('xxx').value)-el.offsetLeft;
      var y = parseInt(document.getElementById('yyy').value)-el.offsetTop;
      
      var x1 = parseInt(x/84*32)
      var y1 = parseInt(y/84*32)
      x = "X:"+ x; 
      y = "Y:"+ y;
      document.getElementById('xxx').value = x1
      document.getElementById('yyy').value = y1
      var arrImg = document.getElementById(el.id+"_show");
      <!--alert(x1+","+y1)-->
      arrImg.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/"+el.id+"_show/"+(x1+1)+"_"+(y1+1)+".jpg"
      
     <!--alert(parseInt(x1/84*32)+","+parseInt(y1/84*32) + ","+el.id+"_show" + ","+arrImg.src);-->
     <!--ChangeImgSrcById(el.id, x1,y1);-->
  }
    

function Show_select(val){
      var arrImg = document.getElementById("Eignvalue_result");
      var arrImg1 = document.getElementById("dynamic_result");
      <!--alert(arrImg)-->
      if(val == 1){
      	arrImg.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/circular-motion.png"
        arrImg1.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_circle_connect.gif"
        }
      else if(val == 2){
       	arrImg.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/cropped-circulat-motion.png"
        arrImg1.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_crop_circle_connect.gif"
       }
      else{
      	arrImg.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/3d-2d-motion.png"
        arrImg1.src = "https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_sph_connect.gif"
      }
     
  }
     
    <!-- ChangeImgSrcWithoutId(); -->
</script>
<!-- define script -->

  <h1>Self-Supervised Decomposition, Disentanglement and Prediction of Video Sequences while Interpreting Dynamics:A Koopman Perspective</h1>

<div class="menu">
<ul class="list_inline">
  <li><a href="https://www.linkedin.com/in/armandcomas/">Armand Comas</a><br>Northeastern university 
  </li>
  <li><a href="https://www.linkedin.com/in/sandesh-ghimire/">Sandesh Ghimire</a><br>Northeastern university
  </li>
  <li><a href="https://www.linkedin.com/in/%E6%98%8A%E9%9C%96-%E6%9D%8E-502ab5118/">Haolin Li</a><br>Northeastern university
  </li>
  <li><a href="https://coe.northeastern.edu/people/camps-octavia/">Octavia Camps*</a><br>Northeastern university
  </li>
</ul>
</div>

<div class="scrollmenu">
<a href="https://www.overleaf.com/project/602a9b6ceeecb42dfaba0453#">Paper</a>
<a href="https://github.com/">Code</a>
<a href="http://robustsystems.coe.neu.edu/">Lab Page</a>
</div>

<h2>Abstract</h2>
<p align = "justify">Human interpretation of the world encompasses the use
of symbols to categorize sensory inputs, and compose them
in a hierarchical manner to build an understanding. One of
the long term objective of computer vision and artifical intelligence is to endow a machine with similar compositionality, structure and interpretation of the world. Towards this
goal, recent methods have successfully been able to decompose and disentangle video sequences into their composing
objects and dynamics, in a self-supervised fashion. However, there has been scarce effort in giving interpretation to
the dynamics of the scene. We propose a method to decompose a video into moving objects and their attributes, and
model each object’s dynamics with linear system identification tools, by means of a Koopman embedding. This allows interpretation, manipulation, and extrapolation of the
dynamics of the different objects by employing the Koopman operator K. We test our method in various synthetic
datasets and successfully forecast challenging trajectories
while interpreting them.
</p>

<h2>Introduction</h2>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/FiguresICCV.jpg_1.png" width="900" height="500">
<p align = "justify"> The above figure shows overall architecture of DIVE, which takes the input video with missing data, infers the missingness (red), pose(green) and appearance (blue) latent variables.  Two separate decoders reconstruct and predict the future sequences.</p>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/1-self_attention_part.png" width="770" height="280"><br><a href="http://jalammar.github.io/illustrated-transformer/">Self attention</a> tracking part.<br><br>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/2-.Disranglementpng.png " width="770" height="280"><br>Disentangle step decompose object to confidence, pose and appearance.<br><br>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/3-koopman.png" width="770" height="280"><br> <a href="https://www.mit.edu/~arbabi/research/KoopmanIntro.pdf">Koopman opeator<a> predict next status.<br><br>

<h2>Experiments</h2>
<p>Process of moving mnist dataset</p>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/tracking-moving-mnist.png" width="770" height="280"><br>

<p>Experiments result</p>
<!--
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/circular-motion.png" width="770" height="360"><br> 
<p>example for ddpae circle, left half is ground truth, right half is ddpae result, I can also change it to KIDD compare with ddpae or etc</p>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_circle_connect.gif" width="64" height="64">

<br><br><br>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/cropped-circulat-motion.png" width="770" height="360"><br> 
<p>example for ddpae crop circle, left half is ground truth, right half is ddpae result</p>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_crop_circle_connect.gif" width="64" height="64">

<br><br><br>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/3d-2d-motion.png" width="770" height="360"><br> 
<p>example for ddpae sph, left half is ground truth, right half is ddpae result</p>
<img src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_sph_connect.gif" width="64" height="64">
-->
    
<br><br><br>
<select id="group" value="1" onchange="Show_select(this.value);">
    <option value="1">Angle_1</option>
    <option value="2">Angle_2</option>
    <option value="3">Angle_3</option>
</select>
<br><br>
<img id = "Eignvalue_result" src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/circular-motion.png" width="770" height="380">
<br><br>
<img id = "dynamic_result" src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/ddpae_circle_connect.gif" width="64" height="64">






<p>This part is for interactive website</p>

<!--<div class="sampleImage svelte-1v04nw5" style="height: 84px;padding-left:6px">-->
<body>
    <img id = "first_dynamic" src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/original_image.jpg" width="84" onclick="Show(this)" />
    <br>
    <p>Self attention result</p><br>
    X:<input id="xxx" type="text" />  Y:<input id="yyy" type="text" />
    <br><br>
    <img id = "first_dynamic_show" src="https://github.com/lihaolin1/lihaolin1.github.io/raw/main/img/combine_image.jpg" width="770">
</body>
<!-- </div> -->
    
    
<!--    
<h2>Reference</h2>
<p>
[1] Omri Azencot, N. Benjamin Erichson, Vanessa Lin, and
Michael Mahoney. Forecasting sequential data using consistent koopman autoencoders. In Hal Daume III and Aarti ´
Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 475–485. PMLR, 13–18
Jul 2020. 2, 3
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. CoRR, abs/1409.0473, 2015. 4
[3] S. Brunton, B. W. Brunton, J. Proctor, E. Kaiser, and J. N.
Kutz. Chaos as an intermittently forced linear system. Nature
Communications, 8, 2017. 2, 3
[4] C. Burgess, Lo¨ıc Matthey, Nicholas Watters, Rishabh
Kabra, I. Higgins, M. Botvinick, and Alexander Lerchner.
Monet: Unsupervised scene decomposition and representation. ArXiv, abs/1901.11390, 2019. 1
[5] Kyunghyun Cho, B. V. Merrienboer, C¸ aglar Gulc¸ehre, ¨
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. ArXiv,
abs/1406.1078, 2014. 4
[6] Armand Comas, Chi Zhang, Zlatan Feric, Octavia Camps,
and Rose Yu. Learning disentangled representations of videos with missing data. Advances in Neural Information
Processing Systems, 33, 2020. 1
[7] Emily L Denton and vighnesh Birodkar. Unsupervised learning of disentangled representations from video. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,
Inc., 2017. 1, 3, 5, 8
[8] S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval
Tassa, David Szepesvari, koray kavukcuoglu, and Geoffrey E
Hinton. Attend, infer, repeat: Fast scene understanding with
generative models. In D. Lee, M. Sugiyama, U. Luxburg, I.
Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates,
Inc., 2016. 1, 2
[9] A. Graves, Greg Wayne, M. Reynolds, T. Harley, Ivo Danihelka, Agnieszka Grabska-Barwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, J. Agapiou,
Adria Puigdom ` enech Badia, K. Hermann, Yori Zwols, Georg `
Ostrovski, A. Cain, H. King, C. Summerfield, P. Blunsom,
K. Kavukcuoglu, and Demis Hassabis. Hybrid computing
using a neural network with dynamic external memory. Nature, 538:471–476, 2016. 4
[10] Klaus Greff, Raphael Lopez Kaufman, Rishabh Kabra, Nick ¨
Watters, Christopher Burgess, Daniel Zoran, Loic Matthey,
Matthew Botvinick, and Alexander Lerchner. Multi-object
representation learning with iterative variational inference.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pages 2424–2433. PMLR, 09–15 Jun 2019. 1, 2
[11] Z. He, J. Li, Daxue Liu, Hangen He, and D. Barber. Tracking by animation: Unsupervised learning of multi-object attentive trackers. 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 1318–1327, 2019. 1, 3, 4
[12] Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei,
and Juan Carlos Niebles. Learning to decompose and disentangle representations for video prediction. In S. Bengio, H.
Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 1, 3,5, 8
[13] Jindong Jiang*, Sepehr Janghorbani*, Gerard De Melo, and
Sungjin Ahn. Scalor: Generative world models with scalable object representations. In International Conference on
Learning Representations, 2020. 1, 3, 5, 8
[14] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world models. In International
Conference on Learning Representations, 2020. 1, 3
[15] B. O. Koopman. Hamiltonian systems and transformation
in hilbert space. Proceedings of the National Academy of
Sciences of the United States of America, 17 5:315–8, 1931.2
[16] Adam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar
Posner. Sequential attend, infer, repeat: Generative modelling of moving objects. In S. Bengio, H. Wallach, H.Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. 1, 3
[17] Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, and Kristian Kersting. Structured object-aware physics
prediction for video modeling and planning. In International
Conference on Learning Representations, 2020. 1, 3
[18] Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio
Torralba. Learning compositional koopman operators for
model-based control. In International Conference on Learning Representations, 2020. 2, 3
[19] Z. Lin, Yi-Fu Wu, S. Peri, Weihao Sun, G. Singh, Fei Deng,
Jindong Jiang, and Sungjin Ahn. Space: Unsupervised
object-oriented scene representation via spatial attention and
decomposition. ArXiv, abs/2001.02407, 2020. 1
[20] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Objectcentric learning with slot attention. In H. Larochelle, M.
Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,
Advances in Neural Information Processing Systems, volume 33, pages 11525–11538. Curran Associates, Inc., 2020.
1, 2
[21] Bethany Lusch, J. N. Kutz, and S. Brunton. Deep learning for
universal linear embeddings of nonlinear dynamics. Nature
Communications, 9, 2018. 1, 2, 3
[22] Andreas Mardt, L. Pasquali, Hao Wu, and F. Noe. Vampnets ´
for deep learning of molecular kinetics. Nature Communications, 9, 2017. 2, 3
[23] I. Mezic. Spectral properties of dynamical systems,
model reduction and decompositions. Nonlinear Dynamics,
41:309–325, 2005. 2
[24] Jeremy Morton, Antony Jameson, Mykel J Kochenderfer,
and Freddie Witherden. Deep dynamical modeling and control of unsteady fluid flows. In S. Bengio, H. Wallach, H.
Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
editors, Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018. 2, 3
[25] Jeremy Morton, F. Witherden, and Mykel J. Kochenderfer. Deep variational koopman models: Inferring koopman
observations for uncertainty-aware dynamics modeling and
control. In IJCAI, 2019. 2, 3
[26] Samuel E. Otto and Clarence W. Rowley. Linearly recurrent
autoencoder networks for learning dynamics. SIAM Journal
on Applied Dynamical Systems, 18(1):558–593, 2019. 3
[27] Charles L Phillips and H Troy Nagle. Digital control system
analysis and design. Prentice Hall Press, 2007. 7
[28] J. Proctor, S. Brunton, and J. N. Kutz. Generalizing koopman
theory to allow for inputs and control. SIAM J. Appl. Dyn.
Syst., 17:909–930, 2018. 2, 3
[29] P. Schmid. Dynamic mode decomposition of numerical and
experimental data. Journal of Fluid Mechanics, 656:5–28,
2008. 1, 2, 3
[30] Nitish Srivastava, Elman Mansimov, and R. Salakhutdinov.
Unsupervised learning of video representations using lstms.
In ICML, 2015. 5
[31] M. Williams, I. Kevrekidis, and C. Rowley. A data–driven
approximation of the koopman operator: Extending dynamic mode decomposition. Journal of Nonlinear Science,
25:1307–1346, 2015. 1, 2, 3
[32] Yongqian Xiao, Xin Xu, and Qianli Lin. Cknet: A convolutional neural network based on koopman operator for modeling latent dynamics from pixels. ArXiv, abs/2102.10205,
2021. 2, 3
[33] Tian Xie, A. France-Lanord, Yanming Wang, Y. Shao-Horn,
and J. Grossman. Graph dynamical networks for unsupervised learning of atomic scale dynamics in materials. Nature
Communications, 10, 2019. 2, 3
[34] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR, 2018. 5
</p>
-->
 
<h2>Citation</h2>
<p>If you want to cite our work, please use:</p>
    <pre>
    " @inproceedings{RSL2021KIDD,
          author    = {Armand Sandesh Haolin Octavia},  
          title     = {KIDD: Self-Supervised Decomposition, Disentanglement and Predictionof Video Sequences while Interpreting Dynamics:A Koopman Perspective},
          booktitle   = {},
          year      = {2021},
        }"
    </pre>

<!--<h2>Knowledge</h2> 
<p>This part is for part3</p>-->
